{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15157c49-acd7-4f8a-9242-584919b40962",
   "metadata": {},
   "source": [
    "## Overview of `uresnet.cfg` Configuration File\n",
    "\n",
    "The `uresnet.cfg` configuration file defines how the UResNet model will be trained using **SPINE** for a semantic segmentation task. Below is a breakdown of the key components.\n",
    "\n",
    "### 1. Base Configuration\n",
    "This section defines general settings for the training process:\n",
    "\n",
    "- **`world_size: 1`**: Specifies the number of GPUs to use (1 GPU in this case).\n",
    "- **`iterations: 25000`**: The number of training iterations (~25 epochs).\n",
    "- **`seed: 0`**: A random seed for reproducibility.\n",
    "- **`unwrap: false`**: Do not break down input/output images within the batch (this is not necessary for training).\n",
    "- **`log_dir: logs/uresnet`**: The directory where training logs will be saved.\n",
    "- **`log_step: 1`**: Log training information after every iteration.\n",
    "- **`overwrite_log: true`**: Overwrite existing logs if they already exist.\n",
    "\n",
    "Within the `train` block:\n",
    "- **`weight_prefix: weights/uresnet/snapshot`**: Specifies where to save model weights.\n",
    "- **`save_step: 1000`**: Save the model weights every 1,000 iterations.\n",
    "- **`optimizer` block**:\n",
    "  - **`name: Adam`**: Use the Adam optimizer for training.\n",
    "  - **`lr: 0.001`**: Learning rate for the optimizer.\n",
    "\n",
    "### 2. IO (Input/Output) Configuration\n",
    "This section controls how the data will be loaded for training:\n",
    "\n",
    "- **`loader` block**:\n",
    "  - **`batch_size: 128`**: The size of each training batch.\n",
    "  - **`shuffle: false`**: Data shuffling is disabled (randomization is done via a sampler).\n",
    "  - **`num_workers: 4`**: Number of CPU cores used for data loading.\n",
    "  - **`collate_fn: all`**: Specifies how to combine data samples into batches.\n",
    "  - **`sampler: random_sequence`**: Load data using a random sequence.\n",
    "  \n",
    "- **`dataset` block**:\n",
    "  - **`name: larcv`**: Specifies the dataset format.\n",
    "  - **`file_keys: /sdf/data/neutrino/generic/mpvmpr_2020_01_v04/train.root`**: Path to the training dataset.\n",
    "  - **`schema` block**:\n",
    "    - **`data`**:\n",
    "      - **`parser: sparse3d`**: Specifies that the data format is sparse3D.\n",
    "      - **`sparse_event: sparse3d_pcluster`**: Label for sparse 3D cluster data.\n",
    "    - **`seg_label`**:\n",
    "      - **`parser: sparse3d`**: Specifies the format for segmentation labels.\n",
    "      - **`sparse_event: sparse3d_pcluster_semantics`**: Label for sparse 3D segmentation.\n",
    "\n",
    "### 3. Model Configuration\n",
    "This section defines the UResNet model:\n",
    "\n",
    "- **`name: uresnet`**: Specifies the UResNet model to be used.\n",
    "- **`weight_path: null`**: No pretrained weights are used for this model.\n",
    "  \n",
    "- **`network_input` block**:\n",
    "  - **`data: data`**: Specifies the input data for the network (defined in the `IO` block).\n",
    "  \n",
    "- **`loss_input` block**:\n",
    "  - **`seg_label: seg_label`**: Specifies the segmentation labels used in the loss function.\n",
    "\n",
    "### 4. Modules\n",
    "This section describes the architecture of the UResNet model and its loss function:\n",
    "\n",
    "- **`uresnet` block**:\n",
    "  - **`num_input: 1`**: Number of input channels (1 for grayscale images).\n",
    "  - **`num_classes: 5`**: The number of output classes.\n",
    "  - **`filters: 32`**: Number of filters in the first convolutional layer.\n",
    "  - **`depth: 5`**: The depth of the UResNet architecture (5 layers).\n",
    "  - **`reps: 2`**: Each layer is repeated twice.\n",
    "  - **`allow_bias: false`**: No bias term is used in the convolutional layers.\n",
    "  - **`activation` block**:\n",
    "    - **`name: lrelu`**: Leaky ReLU activation function.\n",
    "    - **`negative_slope: 0.33`**: Slope for the Leaky ReLU function.\n",
    "  - **`norm_layer` block**:\n",
    "    - **`name: batch_norm`**: Use batch normalization.\n",
    "    - **`eps: 0.0001`**: Epsilon value to prevent division by zero.\n",
    "    - **`momentum: 0.01`**: Momentum for the moving average in batch normalization.\n",
    "  \n",
    "- **`uresnet_loss` block**:\n",
    "  - **`balance_loss: false`**: The loss function is not balanced across different classes.\n",
    "\n",
    "### Purpose\n",
    "The `uresnet.cfg` file defines the training process for a **UResNet model**, which is used for **3D sparse image segmentation**. The model is configured to run for 25,000 iterations, with model checkpoints saved every 1,000 iterations. It also logs the training progress after each iteration and uses the Adam optimizer to adjust the model weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c7383-790f-4f6e-a263-00e51e04e3e8",
   "metadata": {},
   "source": [
    "## Overview of `uresnet_val.cfg` Configuration File\n",
    "\n",
    "The `uresnet_val.cfg` file defines how the UResNet model will be validated using **SPINE**. It shares many similarities with the training configuration but is tailored for the validation process.\n",
    "\n",
    "### 1. Base Configuration\n",
    "This section defines general settings for the validation process:\n",
    "\n",
    "- **`world_size: 1`**: Specifies the number of GPUs to use (1 GPU in this case).\n",
    "- **`iterations: 100`**: The number of iterations for validation (~0.5 epochs).\n",
    "- **`seed: 0`**: A random seed for reproducibility.\n",
    "- **`unwrap: false`**: This setting should remain `false` during validation to avoid unnecessary operations.\n",
    "- **`log_dir: logs/uresnet`**: Specifies the directory where logs will be saved.\n",
    "- **`log_step: 1`**: Log validation information after every iteration.\n",
    "- **`overwrite_log: true`**: If set to `true`, existing logs will be overwritten.\n",
    "\n",
    "### 2. IO (Input/Output) Configuration\n",
    "This section controls how the data is loaded for validation:\n",
    "\n",
    "- **`loader` block**:\n",
    "  - **`batch_size: 128`**: Specifies the size of each validation batch.\n",
    "  - **`shuffle: false`**: Shuffling is turned off for validation to ensure deterministic evaluation.\n",
    "  - **`num_workers: 4`**: Number of CPU cores used for data loading.\n",
    "  - **`collate_fn: all`**: Specifies how data samples are combined into batches.\n",
    "  \n",
    "- **`dataset` block**:\n",
    "  - **`name: larcv`**: Specifies the format of the dataset.\n",
    "  - **`file_keys: /sdf/data/neutrino/generic/mpvmpr_2020_01_v04/test.root`**: Path to the validation dataset.\n",
    "  - **`schema` block**:\n",
    "    - **`data`**:\n",
    "      - **`parser: sparse3d`**: Specifies how to parse the data (sparse3D format).\n",
    "      - **`sparse_event: sparse3d_pcluster`**: Label for sparse 3D cluster data.\n",
    "    - **`seg_label`**:\n",
    "      - **`parser: sparse3d`**: Specifies how to parse segmentation labels.\n",
    "      - **`sparse_event: sparse3d_pcluster_semantics`**: Label for sparse 3D segmentation labels.\n",
    "\n",
    "### 3. Model Configuration\n",
    "This section defines the model and how the validation will use it:\n",
    "\n",
    "- **`name: uresnet`**: Specifies the UResNet model.\n",
    "- **`weight_path: weights/uresnet/snapshot*.ckpt`**: Specifies the path to the saved model weights that will be used for validation.\n",
    "\n",
    "- **`network_input` block**:\n",
    "  - **`data: data`**: Defines the input data for the network.\n",
    "  \n",
    "- **`loss_input` block**:\n",
    "  - **`seg_label: seg_label`**: Specifies the segmentation labels for validation.\n",
    "\n",
    "### 4. Modules\n",
    "This section describes the UResNet model architecture for validation:\n",
    "\n",
    "- **`uresnet` block**:\n",
    "  - **`num_input: 1`**: Number of input channels (1 for grayscale images).\n",
    "  - **`num_classes: 5`**: Number of output classes.\n",
    "  - **`filters: 32`**: Number of filters in the first convolutional layer.\n",
    "  - **`depth: 5`**: The depth of the UResNet architecture (5 layers).\n",
    "  - **`reps: 2`**: Each layer is repeated twice.\n",
    "  - **`allow_bias: false`**: No bias term is used in the convolutional layers.\n",
    "  - **`activation` block**:\n",
    "    - **`name: lrelu`**: Leaky ReLU activation function.\n",
    "    - **`negative_slope: 0.33`**: Slope for the Leaky ReLU function.\n",
    "  - **`norm_layer` block**:\n",
    "    - **`name: batch_norm`**: Applies batch normalization.\n",
    "    - **`eps: 0.0001`**: Epsilon for numerical stability in batch normalization.\n",
    "    - **`momentum: 0.01`**: Momentum for batch normalization.\n",
    "\n",
    "- **`uresnet_loss` block**:\n",
    "  - **`balance_loss: false`**: The loss function is not balanced across different classes.\n",
    "\n",
    "### Purpose\n",
    "The `uresnet_val.cfg` configuration file is specifically designed for validating the performance of a **UResNet model** on a test dataset. It loads previously saved model weights, runs the model for 100 iterations on the test data, and logs the validation results. This configuration ensures that the model's performance is evaluated in a controlled, deterministic environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe502949-e3c5-4c9d-8310-6c089d8d1bfb",
   "metadata": {},
   "source": [
    "## Overview of `grappa_shower.cfg` Configuration File\n",
    "\n",
    "The `grappa_shower.cfg` file defines the training configuration for a **GRAPPA**-based model using **SPINE**. This file configures how the model will process 3D cluster and particle data for training.\n",
    "\n",
    "### 1. Base Configuration\n",
    "This section defines general training settings:\n",
    "\n",
    "- **`world_size: 1`**: Specifies the number of GPUs to use (in this case, 1 GPU).\n",
    "- **`iterations: 25000`**: The total number of training iterations (~25 epochs).\n",
    "- **`seed: 0`**: Random seed for reproducibility.\n",
    "- **`unwrap: false`**: Disables breaking down input/output images in the batch (saves time during training).\n",
    "- **`log_dir: logs`**: Directory where training logs will be stored.\n",
    "- **`log_step: 1`**: Logging frequency, logs will be written every iteration.\n",
    "- **`overwrite_log: true`**: Allows overwriting existing logs.\n",
    "  \n",
    "- **`train` block**:\n",
    "  - **`weight_prefix: weights/grappa_shower/snapshot-`**: Path where model weights will be saved.\n",
    "  - **`save_step: 1000`**: Specifies how frequently to save model weights (every epoch in this case).\n",
    "  - **`optimizer` block**:\n",
    "    - **`name: Adam`**: Specifies the use of the Adam optimizer.\n",
    "    - **`lr: 0.001`**: The learning rate for the optimizer.\n",
    "\n",
    "### 2. IO (Input/Output) Configuration\n",
    "This section defines how data is loaded during training:\n",
    "\n",
    "- **`loader` block**:\n",
    "  - **`batch_size: 128`**: The batch size for training.\n",
    "  - **`shuffle: false`**: Data shuffling is disabled (randomization is handled by a custom sampler).\n",
    "  - **`num_workers: 8`**: Number of CPU cores used for data loading.\n",
    "  - **`collate_fn: all`**: Specifies how to combine individual data samples into batches.\n",
    "  - **`sampler: random_sequence`**: Uses a random sequence sampler for loading data.\n",
    "\n",
    "- **`dataset` block**:\n",
    "  - **`name: larcv`**: Specifies the format of the dataset.\n",
    "  - **`file_keys: /sdf/data/neutrino/generic/mpvmpr_2020_01_v04/train.root`**: Path to the training dataset.\n",
    "  \n",
    "  - **`schema` block**:\n",
    "    - **`data` block**:\n",
    "      - **`parser: cluster3d`**: Specifies how to parse the 3D cluster data.\n",
    "      - **`cluster_event: cluster3d_pcluster`**: Event label for 3D cluster data.\n",
    "      - **`particle_event: particle_corrected`**: Event label for corrected particle data.\n",
    "      - **`sparse_semantics_event: sparse3d_pcluster_semantics`**: Event label for semantic sparse 3D cluster data.\n",
    "      - **`add_particle_info: true`**: Enables adding additional particle information.\n",
    "      - **`clean_data: true`**: Specifies whether to clean the data before processing.\n",
    "      \n",
    "    - **`coord_label` block**:\n",
    "      - **`parser: particle_coords`**: Parser for particle coordinates.\n",
    "      - **`particle_event: particle_corrected`**: Event label for corrected particle data.\n",
    "      - **`cluster_event: cluster3d_pcluster`**: Event label for the 3D cluster data.\n",
    "\n",
    "### 3. Model Configuration\n",
    "This section defines the **GRAPPA** model and its components:\n",
    "\n",
    "- **`name: grappa`**: Specifies the GRAPPA model.\n",
    "- **`weight_path: null`**: No pretrained weights are used for the training.\n",
    "\n",
    "- **`network_input` block**:\n",
    "  - **`data: data`**: Defines the input data for the network.\n",
    "  - **`coord_label: coord_label`**: Specifies the coordinate labels as input.\n",
    "  \n",
    "- **`loss_input` block**:\n",
    "  - **`clust_label: data`**: Specifies the cluster label to be used for calculating loss.\n",
    "\n",
    "### 4. Modules\n",
    "This section defines the specific components of the GRAPPA model and its architecture:\n",
    "\n",
    "- **`grappa` block**:\n",
    "  - **`nodes` block**:\n",
    "    - **`source: cluster`**: Specifies that the source is the cluster data.\n",
    "    - **`shapes: [shower, michel, delta]`**: Specifies the types of shapes to identify: shower, Michel electron, and delta.\n",
    "    - **`min_size: -1`**: Minimum size filter for the clusters (set to -1, no filtering).\n",
    "    - **`make_groups: false`**: Groups are not created automatically.\n",
    "    - **`grouping_method: score`**: Clusters are grouped based on a scoring mechanism.\n",
    "    \n",
    "  - **`graph` block**:\n",
    "    - **`name: complete`**: Defines a complete graph for connecting the nodes.\n",
    "    - **`max_length`**: Maximum distance between nodes for creating edges.\n",
    "\n",
    "  - **`node_encoder` block**:\n",
    "    - **`name: geo`**: Specifies the geometry-based node encoder.\n",
    "    - Various options like `add_value`, `add_shape`, `add_points`, etc. allow adding different geometric features to the node representation.\n",
    "\n",
    "  - **`edge_encoder` block**:\n",
    "    - **`name: geo`**: Specifies a geometry-based edge encoder.\n",
    "\n",
    "  - **`gnn_model` block**:\n",
    "    - **`name: meta`**: Specifies the use of a meta-learning GNN model.\n",
    "    - **`node_feats: 33`**: Number of features for each node.\n",
    "    - **`edge_feats: 19`**: Number of features for each edge.\n",
    "    - **`node_pred: 2`**: Number of output predictions for each node.\n",
    "    - **`edge_pred: 2`**: Number of output predictions for each edge.\n",
    "    - **`edge_layer` block**:\n",
    "      - Specifies a multilayer perceptron (MLP) with 3 layers for predicting edges.\n",
    "    - **`node_layer` block**:\n",
    "      - Specifies an MLP for predicting node states with options for normalization, aggregation, and activation functions.\n",
    "\n",
    "- **`grappa_loss` block**:\n",
    "  - **`node_loss` block**:\n",
    "    - **`name: shower_primary`**: Specifies that the primary loss is for shower segmentation.\n",
    "  - **`edge_loss` block**:\n",
    "    - **`name: channel`**: Specifies the edge loss based on the \"channel\" connection between nodes.\n",
    "\n",
    "### Purpose\n",
    "The `grappa_shower.cfg` file sets up the GRAPPA model for 3D cluster-based training, primarily focusing on recognizing shapes like showers, Michel electrons, and delta rays. The file controls the training process, specifying how the dataset is loaded, how the model is configured, and how the training is performed. It trains the model for 25,000 iterations and saves checkpoints every 1,000 iterations while utilizing various geometric features for both node and edge encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135abc6a-a21a-47d3-8afc-ad79baf6d614",
   "metadata": {},
   "source": [
    "## Overview of `train_uresnet.sh` Script\n",
    "\n",
    "The `train_uresnet.sh` script is a SLURM batch job submission script used to train the **UResNet** model on a GPU cluster. It specifies the job configuration and execution details for running the training process using **Singularity** and **SPINE**.\n",
    "\n",
    "### SLURM Configuration\n",
    "\n",
    "The first part of the script defines the SLURM job options:\n",
    "\n",
    "- **`--account=neutrino:ml-dev`**: Specifies the account to charge the GPU usage to, in this case, the neutrino machine learning development account.\n",
    "- **`--partition=ampere`**: Specifies the partition to run the job in. The \"ampere\" partition corresponds to nodes with **Ampere GPUs**.\n",
    "- **`--job-name=train_uresnet`**: Sets the job name as \"train_uresnet,\" which will be visible in SLURM job listings.\n",
    "- **`--output=batch_outputs/output-train_uresnet.txt`**: Specifies the output file for stdout messages during the job execution.\n",
    "- **`--error=batch_outputs/output-train_uresnet.txt`**: Specifies the output file for stderr messages during the job execution.\n",
    "  \n",
    "- **`--ntasks=1`**: Specifies the number of tasks to run (1 task).\n",
    "- **`--cpus-per-task=5`**: Allocates 5 CPU cores for the task.\n",
    "- **`--mem-per-cpu=4g`**: Allocates 4 GB of RAM per CPU core (total of 20 GB for the job).\n",
    "- **`--time=6:00:00`**: Sets the maximum runtime for the job to 6 hours.\n",
    "- **`--gpus=a100:1`**: Requests 1 A100 GPU for the job.\n",
    "\n",
    "### Singularity and Execution Command\n",
    "\n",
    "The core of the script executes the training using **Singularity**:\n",
    "\n",
    "- **`singularity exec`**: Executes a command within a Singularity container.\n",
    "  - **`--bind /sdf/group/neutrino/drielsma/,/sdf/data/neutrino/`**: Binds the directories to the container, ensuring that the necessary files for the training are accessible from the Singularity environment.\n",
    "  - **`--nv`**: Enables NVIDIA GPU support inside the container.\n",
    "  - **`/sdf/group/neutrino/images/develop.sif`**: Specifies the Singularity image (`develop.sif`) that contains the necessary software and dependencies for training.\n",
    "  \n",
    "- **`bash -c`**: Executes a command within a bash shell.\n",
    "  - **`\"python3 /sdf/data/neutrino/software/spine/bin/run.py -c /sdf/group/neutrino/drielsma/train/example/uresnet.cfg\"`**: \n",
    "    - **`python3`**: Runs the Python interpreter.\n",
    "    - **`/sdf/data/neutrino/software/spine/bin/run.py`**: Specifies the SPINE executable script to run the UResNet training.\n",
    "    - **`-c /sdf/group/neutrino/drielsma/train/example/uresnet.cfg`**: Specifies the configuration file (`uresnet.cfg`) that contains the details for the UResNet model training.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This script automates the submission of a training job for the **UResNet** model to a GPU cluster. It leverages **Singularity** to encapsulate the required environment, and **SLURM** to manage the job scheduling, resource allocation, and execution. The training is configured to use an A100 GPU and is expected to run for a maximum of 6 hours, saving both standard output and error messages to the specified log files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5693b-ec5a-423e-b4ac-f08070e11509",
   "metadata": {},
   "source": [
    "## Overview of `val_uresnet.sh` Script\n",
    "\n",
    "The `val_uresnet.sh` script is a SLURM batch job submission script used to validate the **UResNet** model on a GPU cluster. This script specifies the necessary configuration and execution details for running the validation process using **Singularity** and **SPINE**.\n",
    "\n",
    "### SLURM Configuration\n",
    "\n",
    "The first section of the script contains SLURM job options, which define how the job should be executed on the cluster:\n",
    "\n",
    "- **`--account=neutrino:ml-dev`**: The account under which the GPU usage is charged. Here, it's for the neutrino machine learning development group.\n",
    "- **`--partition=ampere`**: Specifies the partition for the job. The \"ampere\" partition is for nodes with **Ampere GPUs**.\n",
    "- **`--job-name=val_uresnet`**: The job name is set as \"val_uresnet,\" which helps identify this validation job in SLURM job queues.\n",
    "- **`--output=batch_outputs/output-val_uresnet.txt`**: Defines where the output log (stdout) will be saved.\n",
    "- **`--error=batch_outputs/output-val_uresnet.txt`**: Defines where the error log (stderr) will be saved.\n",
    "  \n",
    "- **`--ntasks=1`**: Specifies that only 1 task will be executed.\n",
    "- **`--cpus-per-task=5`**: Allocates 5 CPU cores for this task (which helps in loading and processing data).\n",
    "- **`--mem-per-cpu=4g`**: Allocates 4 GB of RAM per CPU core, giving a total of 20 GB for the job.\n",
    "- **`--time=0:15:00`**: Sets the maximum runtime to 15 minutes, which is sufficient for the validation process.\n",
    "- **`--gpus=a100:1`**: Requests 1 A100 GPU for the job, which is required to run the model.\n",
    "\n",
    "### Singularity and Execution Command\n",
    "\n",
    "The core of the script runs the validation using **Singularity**:\n",
    "\n",
    "- **`singularity exec`**: Executes the validation process inside a Singularity container.\n",
    "  - **`--bind /sdf/group/neutrino/drielsma/,/sdf/data/neutrino/`**: Binds the necessary directories, allowing access to the data and configuration files inside the Singularity container.\n",
    "  - **`--nv`**: Enables the use of Nvidia GPUs within the container.\n",
    "  - **`/sdf/group/neutrino/images/develop.sif`**: Points to the Singularity image (`develop.sif`), which contains the required software and dependencies for running the UResNet validation.\n",
    "\n",
    "- **`bash -c`**: Executes the command in a bash shell:\n",
    "  - **`python3 /sdf/data/neutrino/software/spine/bin/run.py -c /sdf/group/neutrino/drielsma/train/example/uresnet_val.cfg`**: \n",
    "    - **`python3`**: Runs the Python interpreter.\n",
    "    - **`/sdf/data/neutrino/software/spine/bin/run.py`**: Specifies the SPINE executable script to run the UResNet validation.\n",
    "    - **`-c /sdf/group/neutrino/drielsma/train/example/uresnet_val.cfg`**: Specifies the validation configuration file (`uresnet_val.cfg`), which contains the details for running the UResNet validation process.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This script automates the submission of a validation job for the **UResNet** model on a GPU cluster. The job runs for 15 minutes, validating the model using the pre-trained weights, and saving both standard output and error logs to specific files. The validation process is run inside a Singularity container, ensuring that all dependencies are properly managed, while **SLURM** handles the scheduling and resource allocation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95ace4-72d7-4f8e-8ed3-5cac8040b1b9",
   "metadata": {},
   "source": [
    "# Workflow Explanation of Training and Validation Configuration Files\n",
    "\n",
    "This document explains the workflow of the configuration files (`uresnet.cfg`, `uresnet_val.cfg`, `grappa_shower.cfg`, `train_uresnet.sh`, and `val_uresnet.sh`) in conjunction with the **Training_validation.ipynb** notebook.\n",
    "\n",
    "## 1. Training_validation.ipynb\n",
    "\n",
    "This notebook serves as a detailed guide to understanding the various configuration files used for training and validating machine learning models in the **SPINE** framework. It walks through the basic concepts of the configuration files and how to execute the training and validation processes on a GPU cluster. The notebook focuses on **UResNet**, a deep neural network architecture used for semantic segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. uresnet.cfg\n",
    "\n",
    "### Purpose:\n",
    "This file is used to define the configuration for training the **UResNet** model on sparse 3D data, likely for a neutrino detection task. It specifies how the data is loaded, how the model is structured, and how training is performed.\n",
    "\n",
    "### Workflow:\n",
    "1. **Base Configuration**:\n",
    "   - The training iterations, logging directory, and the optimizer (Adam with a learning rate of 0.001) are specified.\n",
    "   - The UResNet model is set to train for 25,000 iterations, saving checkpoints every 1,000 iterations.\n",
    "\n",
    "2. **IO Configuration**:\n",
    "   - The data is loaded from a sparse 3D dataset located at `/sdf/data/neutrino/generic/mpvmpr_2020_01_v04/train.root`.\n",
    "   - The data loader prepares the batch size (128) and uses 4 CPU cores to load data efficiently.\n",
    "\n",
    "3. **Model Configuration**:\n",
    "   - The model is configured as **UResNet**, with the architecture specified in terms of layers, filters, and activation functions.\n",
    "   - No pre-trained weights are used, and the model is trained from scratch.\n",
    "\n",
    "### Execution:\n",
    "- This configuration file is loaded in the **Training_validation.ipynb** notebook to guide the training process. In the notebook, this configuration is discussed and broken down into smaller pieces for better understanding (Cells 4 to 6).\n",
    "- The training is triggered using `train_uresnet.sh`, which submits a job to the SLURM scheduler for GPU-based training.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. uresnet_val.cfg\n",
    "\n",
    "### Purpose:\n",
    "This file is used for validating the UResNet model using previously saved weights. It is similar to the `uresnet.cfg` but with important modifications for the validation process.\n",
    "\n",
    "### Workflow:\n",
    "1. **Base Configuration**:\n",
    "   - The number of iterations is set to 100 (since the model is being validated, not trained), and the log directory remains the same as for training.\n",
    "\n",
    "2. **IO Configuration**:\n",
    "   - The data is now loaded from a **test set** at `/sdf/data/neutrino/generic/mpvmpr_2020_01_v04/test.root` to ensure that the model is being validated on unseen data.\n",
    "\n",
    "3. **Model Configuration**:\n",
    "   - The model architecture is the same as in the training configuration, but now the path to the pre-trained model weights (`weights/uresnet/snapshot*.ckpt`) is specified for the validation process.\n",
    "\n",
    "### Execution:\n",
    "- After the training process is complete, this configuration file is used to validate the model. The notebook guides the process of setting up this configuration (Cell 28), and the validation job is submitted via the `val_uresnet.sh` script, which runs the validation using **Singularity** on a GPU.\n",
    "- The validation logs are automatically stored in the log directory specified (`logs/uresnet`).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. grappa_shower.cfg\n",
    "\n",
    "### Purpose:\n",
    "This configuration file is used to train another model, **Grappa**, which is likely focused on particle clustering or shower detection tasks, with data structured differently than UResNet.\n",
    "\n",
    "### Workflow:\n",
    "1. **Base Configuration**:\n",
    "   - The training is set for 25,000 iterations, saving weights every 1,000 steps, similar to the UResNet configuration. \n",
    "\n",
    "2. **IO Configuration**:\n",
    "   - The dataset is specified as a 3D cluster dataset (`cluster3d`), with additional particle information such as corrected particle events and clean data.\n",
    "\n",
    "3. **Model Configuration**:\n",
    "   - The **Grappa** model is defined with specific node and edge encoders and a GNN-based architecture to process the input data.\n",
    "   - The configuration is tailored for handling shower, Michel, and delta particle shapes.\n",
    "\n",
    "### Execution:\n",
    "- This configuration can be run in a similar manner to the UResNet training, by modifying the script or using a new SLURM script to handle the training process for this model.\n",
    "- Like UResNet, it uses the same general workflow: data loading, model training, and logging.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. train_uresnet.sh\n",
    "\n",
    "### Purpose:\n",
    "This SLURM job submission script automates the process of training the UResNet model on the cluster.\n",
    "\n",
    "### Workflow:\n",
    "1. The script sets up the required job resources, including:\n",
    "   - Allocation of 5 CPU cores and 1 A100 GPU for the job.\n",
    "   - Setting the job name (`train_uresnet`) and output/error log paths.\n",
    "   - Running the job for a maximum of 6 hours.\n",
    "\n",
    "2. The script launches the UResNet training by running the **SPINE** framework inside a **Singularity** container, binding the required directories and executing the training using the `uresnet.cfg` configuration file.\n",
    "\n",
    "### Execution:\n",
    "- The script is submitted using the `sbatch train_uresnet.sh` command, which schedules the training job on the cluster.\n",
    "- The output and error logs can be monitored to check the progress of the training process.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. val_uresnet.sh\n",
    "\n",
    "### Purpose:\n",
    "This script is similar to `train_uresnet.sh`, but it is used for validating the UResNet model on the test dataset.\n",
    "\n",
    "### Workflow:\n",
    "1. The SLURM job is set up similarly, with a shorter time limit of 15 minutes, as validation typically requires fewer resources and time compared to training.\n",
    "\n",
    "2. The script runs the validation using the `uresnet_val.cfg` configuration file inside a **Singularity** container, loading the saved model weights for validation on the test set.\n",
    "\n",
    "### Execution:\n",
    "- This script is executed with the `sbatch val_uresnet.sh` command, running the validation process.\n",
    "- The validation logs are saved, and the validation results (accuracy, loss, etc.) are analyzed.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Workflow Integration\n",
    "\n",
    "### 1. Training (UResNet or Grappa):\n",
    "- The training configuration files (`uresnet.cfg` or `grappa_shower.cfg`) are loaded, specifying how data is handled and how the model is trained.\n",
    "- The training process is initiated by submitting the `train_uresnet.sh` script, which schedules the job on the cluster.\n",
    "- Training logs are generated and saved to monitor the model’s progress.\n",
    "\n",
    "### 2. Validation:\n",
    "- Once the training is complete, the validation configuration (`uresnet_val.cfg`) is used to test the model's performance on the test dataset.\n",
    "- The `val_uresnet.sh` script submits the validation job to SLURM, and the validation logs are saved.\n",
    "\n",
    "### 3. Analysis (via Notebook):\n",
    "- The **Training_validation.ipynb** notebook is used to visualize the training and validation logs, analyze the loss and accuracy, and ensure that the model is learning properly.\n",
    "- The notebook also provides step-by-step instructions for adjusting configurations and submitting jobs.\n",
    "\n",
    "---\n",
    "\n",
    "This workflow allows for a streamlined process of training, validating, and analyzing deep learning models like UResNet in a high-performance computing environment, utilizing SLURM, Singularity, and SPINE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3349abb-fb95-43e5-811d-a235d82e1681",
   "metadata": {},
   "source": [
    "# Workflow Adjustments for Argonne Polaris\n",
    "\n",
    "Since you're working on Argonne Polaris and can download files from SDF, here’s a list of files you need to download and the changes you need to make to point to your new data location on Polaris.\n",
    "\n",
    "## Files to Download from SDF\n",
    "\n",
    "1. **Training Data**\n",
    "   - From: `/sdf/data/neutrino/generic/mpvmpr_2020_01_v04/train.root`\n",
    "   - To: Polaris local directory\n",
    "\n",
    "2. **Validation Data**\n",
    "   - From: `/sdf/data/neutrino/generic/mpvmpr_2020_01_v04/test.root`\n",
    "   - To: Polaris local directory\n",
    "\n",
    "3. **Configuration Files**\n",
    "   - Download these configuration and shell script files:\n",
    "     - UResNet: `uresnet.cfg` and `uresnet_val.cfg`\n",
    "     - GRAPPA: `grappa_shower.cfg`\n",
    "     - Shell scripts: `train_uresnet.sh`, `val_uresnet.sh`\n",
    "\n",
    "## Changes to Make for Polaris\n",
    "\n",
    "In the configuration files (`uresnet.cfg`, `uresnet_val.cfg`, `grappa_shower.cfg`), modify the paths to reflect the new location of your data on Polaris.\n",
    "\n",
    "### 1. uresnet.cfg\n",
    "Original:\n",
    "```yaml\n",
    "file_keys: /sdf/data/neutrino/generic/mpvmpr_2020_01_v04/train.root\n",
    "```\n",
    "Change to:\n",
    "```yaml\n",
    "file_keys: /your/polaris/local/path/train.root\n",
    "```\n",
    "\n",
    "### 2. uresnet.cfg\n",
    "Original:\n",
    "```yaml\n",
    "file_keys: /sdf/data/neutrino/generic/mpvmpr_2020_01_v04/test.root\n",
    "```\n",
    "Change to:\n",
    "```yaml\n",
    "file_keys: /your/polaris/local/path/test.root\n",
    "```\n",
    "\n",
    "### 3. grappa_shower.cfg\n",
    "Original:\n",
    "```yaml\n",
    "file_keys: /sdf/data/neutrino/generic/mpvmpr_2020_01_v04/train.root\n",
    "```\n",
    "Change to:\n",
    "```yaml\n",
    "file_keys: /your/polaris/local/path/train.root\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa35dee-b8e4-40b4-b0af-c5ebea261761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
